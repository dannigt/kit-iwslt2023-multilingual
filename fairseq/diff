diff --git a/fairseq/criterions/label_smoothed_cross_entropy.py b/fairseq/criterions/label_smoothed_cross_entropy.py
index 56d63e3e..d608fb58 100644
--- a/fairseq/criterions/label_smoothed_cross_entropy.py
+++ b/fairseq/criterions/label_smoothed_cross_entropy.py
@@ -98,12 +98,14 @@ class LabelSmoothedCrossEntropyCriterion(FairseqCriterion):
         lprobs = model.get_normalized_probs(net_output, log_probs=True)
         target = model.get_targets(sample, net_output)
         if self.ignore_prefix_size > 0:
-            if getattr(lprobs, "batch_first", False):
-                lprobs = lprobs[:, self.ignore_prefix_size :, :].contiguous()
-                target = target[:, self.ignore_prefix_size :].contiguous()
-            else:
-                lprobs = lprobs[self.ignore_prefix_size :, :, :].contiguous()
-                target = target[self.ignore_prefix_size :, :].contiguous()
+            # if getattr(lprobs, "batch_first", False):
+            lprobs = lprobs[:, self.ignore_prefix_size :, :].contiguous() 
+            target = target[:, self.ignore_prefix_size :].contiguous()
+
+            print(lprobs.shape)
+            # else:
+            #     lprobs = lprobs[self.ignore_prefix_size :, :, :].contiguous()
+            #     target = target[self.ignore_prefix_size :, :].contiguous()
         return lprobs.view(-1, lprobs.size(-1)), target.view(-1)
 
     def compute_loss(self, model, net_output, sample, reduce=True):
diff --git a/fairseq/data/concat_dataset.py b/fairseq/data/concat_dataset.py
index 01a4078b..6ac70e50 100644
--- a/fairseq/data/concat_dataset.py
+++ b/fairseq/data/concat_dataset.py
@@ -122,3 +122,15 @@ class ConcatDataset(FairseqDataset):
         for ds in self.datasets:
             if hasattr(ds, "set_epoch"):
                 ds.set_epoch(epoch)
+
+    def ordered_indices_per_dataset(self):
+        """Return a list of ordered indices vectors for each underlying dataset
+        (with parent dataset indices)."""
+        ordered_indices_list = []
+        for i, dataset in enumerate(self.datasets):
+            start = 0 if i == 0 else self.cumulative_sizes[i - 1]
+            subdataset_indices_list = dataset.ordered_indices_per_dataset()
+            for indices in subdataset_indices_list:
+                ordered_indices_list.append(indices + start)
+
+        return ordered_indices_list
diff --git a/fairseq/data/fairseq_dataset.py b/fairseq/data/fairseq_dataset.py
index 23e6992d..ec3650a9 100644
--- a/fairseq/data/fairseq_dataset.py
+++ b/fairseq/data/fairseq_dataset.py
@@ -189,6 +189,11 @@ class FairseqDataset(torch.utils.data.Dataset, EpochListening):
             )
         return indices, ignored
 
+    def ordered_indices_per_dataset(self):
+        """Return a list of ordered indices vectors for each underlying dataset
+        (with parent dataset indices)."""
+        return [self.ordered_indices()]
+
     @property
     def supports_fetch_outside_dataloader(self):
         """Whether this dataset supports fetching outside the workers of the dataloader."""
diff --git a/fairseq/data/language_pair_dataset.py b/fairseq/data/language_pair_dataset.py
index ff3e14bf..cd626ae6 100644
--- a/fairseq/data/language_pair_dataset.py
+++ b/fairseq/data/language_pair_dataset.py
@@ -159,6 +159,15 @@ def collate(
             constraints[i, 0 : lens[i]] = samples[i].get("constraints")
         batch["constraints"] = constraints.index_select(0, sort_order)
 
+    if samples[0].get("src_lang_id", None) is not None:
+        batch["net_input"]["src_lang_id"] = torch.LongTensor(
+            [s["src_lang_id"] for s in samples]
+        ).unsqueeze(1)
+    if samples[0].get("tgt_lang_id", None) is not None:
+        batch["net_input"]["tgt_lang_id"] = torch.LongTensor(
+            [s["tgt_lang_id"] for s in samples]
+        ).unsqueeze(1)
+
     return batch
 
 
@@ -333,6 +342,10 @@ class LanguagePairDataset(FairseqDataset):
             example["alignment"] = self.align_dataset[index]
         if self.constraints is not None:
             example["constraints"] = self.constraints[index]
+        if self.src_lang_id is not None:
+            example["src_lang_id"] = self.src_lang_id
+        if self.tgt_lang_id is not None:
+            example["tgt_lang_id"] = self.tgt_lang_id
         return example
 
     def __len__(self):
@@ -384,17 +397,7 @@ class LanguagePairDataset(FairseqDataset):
             pad_to_length=pad_to_length,
             pad_to_multiple=self.pad_to_multiple,
         )
-        if self.src_lang_id is not None or self.tgt_lang_id is not None:
-            src_tokens = res["net_input"]["src_tokens"]
-            bsz = src_tokens.size(0)
-            if self.src_lang_id is not None:
-                res["net_input"]["src_lang_id"] = (
-                    torch.LongTensor([[self.src_lang_id]]).expand(bsz, 1).to(src_tokens)
-                )
-            if self.tgt_lang_id is not None:
-                res["tgt_lang_id"] = (
-                    torch.LongTensor([[self.tgt_lang_id]]).expand(bsz, 1).to(src_tokens)
-                )
+
         return res
 
     def num_tokens(self, index):
diff --git a/fairseq/data/multilingual/sampled_multi_dataset.py b/fairseq/data/multilingual/sampled_multi_dataset.py
index b0a61742..21c3303f 100644
--- a/fairseq/data/multilingual/sampled_multi_dataset.py
+++ b/fairseq/data/multilingual/sampled_multi_dataset.py
@@ -327,6 +327,14 @@ class SampledMultiDataset(FairseqDataset):
                 batch["tgt_lang_id"] = straight_order(
                     [b["tgt_lang_id"] for b in batches]
                 )
+
+            if "tgt_lang_id" in batches[0]["net_input"]:
+                # we need the tgt_lang_id in the net_input for indexing
+                # the decoder multilingual adapters
+                batch["net_input"]["tgt_lang_id"] = straight_order(
+                    [b["net_input"]["tgt_lang_id"] for b in batches]
+                )
+
         return batch
 
     @property
@@ -465,3 +473,34 @@ class SampledMultiDataset(FairseqDataset):
         return data_utils.filter_paired_dataset_indices_by_size(
             src_sizes, tgt_sizes, indices, max_sizes
         )
+
+    def ordered_indices_per_dataset(self):
+        """Return a list of ordered indices vectors for each underlying dataset
+        (with parent dataset indices)."""
+        assert self.cumulated_sizes is not None
+        ordered_indices_list = []
+        for i, cumulated_size in enumerate(self.cumulated_sizes):
+            start = 0 if i == 0 else self.cumulated_sizes[i - 1]
+            end = cumulated_size
+
+            indices = np.arange(start, end, dtype=np.int64)
+            if self.shuffle:
+                np.random.shuffle(indices)
+
+            sizes = self.sizes
+            tgt_sizes = (
+                sizes[:, 1] if len(sizes.shape) > 0 and sizes.shape[1] > 1 else None
+            )
+            src_sizes = (
+                sizes[:, 0] if len(sizes.shape) > 0 and sizes.shape[1] > 1 else sizes
+            )
+
+            # sort by target length, then source length
+            if tgt_sizes is not None:
+                indices = indices[np.argsort(tgt_sizes[indices], kind="mergesort")]
+            sort_indices = indices[np.argsort(src_sizes[indices], kind="mergesort")]
+
+            ordered_indices_list.append(sort_indices)
+
+        return ordered_indices_list
+
diff --git a/fairseq/data/multilingual/sampled_multi_epoch_dataset.py b/fairseq/data/multilingual/sampled_multi_epoch_dataset.py
index 17387b2f..cc0eec94 100644
--- a/fairseq/data/multilingual/sampled_multi_epoch_dataset.py
+++ b/fairseq/data/multilingual/sampled_multi_epoch_dataset.py
@@ -197,3 +197,40 @@ class SampledMultiEpochDataset(SampledMultiDataset):
         )
         self._epoch_sizes = None
         self._current_epoch_start_index = index
+
+    def ordered_indices_per_dataset(self):
+        """Return a list of ordered indices vectors for each underlying dataset
+        (with parent dataset indices)."""
+        assert self.cumulated_sizes is not None
+        ordered_indices_list = []
+
+        global_indices = self._random_global_indices[
+            self._current_epoch_start_index : self._current_epoch_start_index
+            + len(self)
+        ]
+
+        for i, cumulated_size in enumerate(self.cumulated_sizes):
+            start = 0 if i == 0 else self.cumulated_sizes[i - 1]
+            end = cumulated_size
+
+            indices = np.where((global_indices >= start) & (global_indices < end))[0]
+
+            if self.shuffle:
+                np.random.shuffle(indices)
+
+            sizes = self.sizes
+            tgt_sizes = (
+                sizes[:, 1] if len(sizes.shape) > 0 and sizes.shape[1] > 1 else None
+            )
+            src_sizes = (
+                sizes[:, 0] if len(sizes.shape) > 0 and sizes.shape[1] > 1 else sizes
+            )
+
+            # sort by target length, then source length
+            if tgt_sizes is not None:
+                indices = indices[np.argsort(tgt_sizes[indices], kind="mergesort")]
+            sort_indices = indices[np.argsort(src_sizes[indices], kind="mergesort")]
+
+            ordered_indices_list.append(sort_indices)
+
+        return ordered_indices_list
diff --git a/fairseq/models/transformer/transformer_base.py b/fairseq/models/transformer/transformer_base.py
index 810c9b98..fffc6ce2 100644
--- a/fairseq/models/transformer/transformer_base.py
+++ b/fairseq/models/transformer/transformer_base.py
@@ -3,6 +3,7 @@
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+import logging
 from typing import Dict, List, Optional, Tuple
 
 import torch
@@ -16,8 +17,10 @@ from fairseq.models.transformer import (
     TransformerDecoderBase,
     TransformerConfig,
 )
+from fairseq.modules.hyper_adapter import HyperNetwork
 from torch import Tensor
 
+logger = logging.getLogger(__name__)
 
 class TransformerModelBase(FairseqEncoderDecoderModel):
     """
@@ -41,6 +44,16 @@ class TransformerModelBase(FairseqEncoderDecoderModel):
         self.cfg = cfg
         self.supports_align_args = True
 
+        if hasattr(cfg, "adapters") and cfg.adapters: # or cfg.hyper_adapters:
+            assert cfg.one_dataset_per_batch, "(hyper-)adapters require `--one-dataset-per-batch`"
+            assert cfg.enable_lang_ids, "(hyper-)adapters require `--enable-lang-ids`"
+
+            if cfg.freeze_pretrained:
+                for n, p in self.named_parameters():
+                    if not hasattr(p, "hyper") and not hasattr(p, "adapter"):
+                        logger.info(f"Freezing (pretrained) parameter '{n}'")
+                        p.requires_grad = False
+
     @staticmethod
     def add_args(parser):
         """Add model-specific arguments to the parser."""
@@ -49,6 +62,89 @@ class TransformerModelBase(FairseqEncoderDecoderModel):
             parser, TransformerConfig(), delete_default=False, with_prefix=""
         )
 
+        # ------------------------------------------------------------------------------------------------
+        # args for adapter layers
+        # ------------------------------------------------------------------------------------------------
+        parser.add_argument('--adapters', default=False, action='store_true',
+                            help="Add an adapter layer after each Transformer layer.")
+        parser.add_argument('--adapters-multi', default=False, action='store_true',
+                            help="Use multiple different adapters, one per task (e.g., language(-pair)).")
+        parser.add_argument('--adapters-bottle', type=int, metavar='D',
+                            help="The dimensionality of the adapter bottleneck representation.")
+        parser.add_argument('--adapters-activation-fn', type=str, default="relu",
+                            choices=utils.get_available_activation_fns(),
+                            help="Activation function for the adapters bottleneck.")
+        parser.add_argument('--adapters-static-layernorm',  default=False, action='store_true',
+                            help="Use LayerNorm without trainable parameters in adapter layers.")
+        parser.add_argument('--adapters-encoder-lang', default="src", type=str,
+                            choices=['src', 'tgt', 'pair'],
+                            help='"The key to use for indexing the encoder language adapters."')
+        parser.add_argument('--adapters-decoder-lang', default="tgt", type=str,
+                            choices=['src', 'tgt', 'pair'],
+                            help="The key to use for indexing the decoder language adapters.")
+        parser.add_argument('--adapters-efficient', default=False, action='store_true',
+                            help="Use a single weight matrix for all adapter layers. "
+                                 "Improves distributed training efficiency.")
+
+        # ------------------------------------------------------------------------------------------------
+        # args for hyper-adapter layers
+        # ------------------------------------------------------------------------------------------------
+        parser.add_argument('--hyper-adapters', default=False, action='store_true',
+                            help="Add a hyper-adapter layer after each Transformer layer.")
+        parser.add_argument('--hyper-adapters-bottle', type=int, metavar='D',
+                            help="The dimensionality of the generated adapter bottleneck representation.")
+        parser.add_argument('--hyper-adapters-hidden-dim', type=int, metavar='D',
+                            help="The dimensionality of the hyper-network hidden representations.")
+        parser.add_argument('--hyper-adapters-hidden-layers', type=int, metavar='D', default=1,
+                            help="The number of hidden layers in the hyper-network.")
+        parser.add_argument('--hyper-adapters-lang-embed-dim', type=int, metavar='D',
+                            help="The dimensionality of the language embeddings of the hyper-network.")
+        parser.add_argument('--hyper-adapters-layer-embed-dim', type=int, metavar='D',
+                            help="The dimensionality of the layer embeddings of the hyper-network.")
+
+        parser.add_argument('--hyper-adapters-dropout', type=float, metavar='D', default=0.0,
+                            help="Dropout used in the hyper-network.")
+        parser.add_argument('--hyper-adapters-activation-fn', type=str, default="relu",
+                            choices=utils.get_available_activation_fns(),
+                            help="Activation function for the hyper-network "
+                                 "and the generated adapters bottleneck.")
+        parser.add_argument('--hyper-adapters-lang-embed-tied', default=False, action='store_true',
+                            help="Use a share embedding matrix for the source and target language embeddings.")
+        parser.add_argument('--hyper-adapters-layernorm-input', default=False, action='store_true',
+                            help="Apply layer normalization to the hyper-network input (lang+layer embeddings).")
+        parser.add_argument('--hyper-adapters-layernorm-output', default=False, action='store_true',
+                            help="Apply layer normalization to the hyper-network output "
+                                 "(before weight generation projections).")
+        parser.add_argument('--hyper-adapters-generate-layernorm', default=False, action='store_true',
+                            help="Generate from the hyper-network the LayerNorm "
+                                 "parameters for each generated adapter layer.")
+        parser.add_argument('--hyper-adapters-init', default="fairseq", type=str, choices=['default', 'hyper'],
+                            help='Initialization method for the weights of the hyper-network.')
+        parser.add_argument('--hyper-adapters-encoder-inputs', default="src,tgt,layer", type=str,
+                            help="Which sources of information to use as input to the "
+                                 "hyper-network when generating the encoder hyper-adapters.")
+        parser.add_argument('--hyper-adapters-decoder-inputs', default="src,tgt,layer", type=str,
+                            help="Which sources of information to use as input to the "
+                                 "hyper-network when generating the decoder hyper-adapters.")
+        parser.add_argument('--hyper-adapters-no-rescale', default=False, action='store_true',
+                            help="Disable the weight rescaling in the hyper-network.")
+
+        # args for finetuning
+        parser.add_argument('--freeze-pretrained', default=False, action='store_true',
+                            help="Freeze the parameters of the main network. "
+                                 "Applicable to experiments for (hyper-)adapters finetuning.")
+
+        # args for analysis
+        parser.add_argument('--network-inspection', default=False, action='store_true',
+                            help="Log and visualize the weights and activations of selected layers.")
+        parser.add_argument('--adapters-lang-swap', type=str, default=None,
+                            help="Swap the (hyper-)adapter of one language with another. "
+                                 "Expects a comma-separated string "
+                                 "(e.g., `--adapter-lang-swap af_ZA-nl_XX,pt_XX-es_XX,gl_ES-pt_XX,uk_UA-ru_RU`)")
+
+        parser.add_argument('--debugging', default=False, action='store_true',
+                            help="debugging")
+
     @classmethod
     def build_model(cls, cfg, task):
         """Build a new model instance."""
@@ -93,8 +189,11 @@ class TransformerModelBase(FairseqEncoderDecoderModel):
             )
         if cfg.offload_activations:
             cfg.checkpoint_activations = True  # offloading implies checkpointing
-        encoder = cls.build_encoder(cfg, src_dict, encoder_embed_tokens)
-        decoder = cls.build_decoder(cfg, tgt_dict, decoder_embed_tokens)
+
+        hypernetwork = cls.build_hypernetwork(cfg, task)
+        encoder = cls.build_encoder(cfg, src_dict, encoder_embed_tokens) #, hypernetwork)
+        decoder = cls.build_decoder(cfg, tgt_dict, decoder_embed_tokens) #, hypernetwork)
+
         if not cfg.share_all_embeddings:
             # fsdp_wrap is a no-op when --ddp-backend != fully_sharded
             encoder = fsdp_wrap(encoder, min_num_params=cfg.min_params_to_wrap)
@@ -114,17 +213,48 @@ class TransformerModelBase(FairseqEncoderDecoderModel):
         return emb
 
     @classmethod
-    def build_encoder(cls, cfg, src_dict, embed_tokens):
-        return TransformerEncoderBase(cfg, src_dict, embed_tokens)
+    def build_encoder(cls, cfg, src_dict, embed_tokens, hypernetwork=None):
+        return TransformerEncoderBase(cfg, src_dict, embed_tokens, hypernetwork=hypernetwork)
 
     @classmethod
-    def build_decoder(cls, cfg, tgt_dict, embed_tokens):
+    def build_decoder(cls, cfg, tgt_dict, embed_tokens, hypernetwork=None):
         return TransformerDecoderBase(
             cfg,
             tgt_dict,
             embed_tokens,
             no_encoder_attn=cfg.no_cross_attention,
+            hypernetwork=hypernetwork
         )
+    @classmethod
+    def build_hypernetwork(cls, cfg, task):
+
+        hypernetwork = None
+#        if cfg.hyper_adapters:
+#            if task.source_dictionary != task.target_dictionary:
+#                raise ValueError("--hyper-adapters requires a joined dictionary")
+#            if cfg.encoder_embed_dim != cfg.decoder_embed_dim:
+#                raise ValueError("--hyper-adapters requires encoder-embed-dim=decoder-embed-dim")
+#
+#            hypernetwork = HyperNetwork(
+#                num_languages=len(cfg.langs),
+#                num_layers=cfg.encoder_layers + cfg.decoder_layers,
+#                lang_embedding_dim=cfg.hyper_adapters_lang_embed_dim,
+#                layer_embedding_dim=cfg.hyper_adapters_layer_embed_dim,
+#                mainnet_input_dim=cfg.encoder_embed_dim,
+#                bottleneck_dim=cfg.hyper_adapters_bottle,
+#                hidden_dim=cfg.hyper_adapters_hidden_dim,
+#                num_hidden_layers=cfg.hyper_adapters_hidden_layers,
+#                dropout=cfg.hyper_adapters_dropout,
+#                activation_fn=cfg.hyper_adapters_activation_fn,
+#                layernorm_input=cfg.hyper_adapters_layernorm_input,
+#                layernorm_output=cfg.hyper_adapters_layernorm_output,
+#                generate_layernorm=cfg.hyper_adapters_generate_layernorm,
+#                language_embedding_tied=cfg.hyper_adapters_lang_embed_tied,
+#                init_method=cfg.hyper_adapters_init,
+#                no_rescale=cfg.hyper_adapters_no_rescale,
+#            )
+
+        return hypernetwork
 
     # TorchScript doesn't support optional arguments with variable length (**kwargs).
     # Current workaround is to add union of all arguments in child classes.
@@ -137,6 +267,8 @@ class TransformerModelBase(FairseqEncoderDecoderModel):
         features_only: bool = False,
         alignment_layer: Optional[int] = None,
         alignment_heads: Optional[int] = None,
+        src_lang_id: Optional[int] = None,
+        tgt_lang_id: Optional[int] = None,
     ):
         """
         Run the forward pass for an encoder-decoder model.
@@ -145,7 +277,9 @@ class TransformerModelBase(FairseqEncoderDecoderModel):
         which are not supported by TorchScript.
         """
         encoder_out = self.encoder(
-            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens
+            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens,
+            src_lang_id=src_lang_id,
+            tgt_lang_id=tgt_lang_id
         )
         decoder_out = self.decoder(
             prev_output_tokens,
@@ -155,6 +289,8 @@ class TransformerModelBase(FairseqEncoderDecoderModel):
             alignment_heads=alignment_heads,
             src_lengths=src_lengths,
             return_all_hiddens=return_all_hiddens,
+            src_lang_id=src_lang_id,
+            tgt_lang_id=tgt_lang_id
         )
         return decoder_out
 
diff --git a/fairseq/models/transformer/transformer_decoder.py b/fairseq/models/transformer/transformer_decoder.py
index 49e37917..beded2fd 100644
--- a/fairseq/models/transformer/transformer_decoder.py
+++ b/fairseq/models/transformer/transformer_decoder.py
@@ -9,9 +9,13 @@ from typing import Any, Dict, List, Optional
 import torch
 import torch.nn as nn
 from fairseq import utils
+from fairseq.data.multilingual.multilingual_data_manager import \
+    MultilingualDatasetManager
 from fairseq.distributed import fsdp_wrap
 from fairseq.models import FairseqIncrementalDecoder
 from fairseq.models.transformer import TransformerConfig
+from fairseq.models.transformer.adapter_helpers import \
+    add_new_layers_to_pretrained
 from fairseq.modules import (
     AdaptiveSoftmax,
     BaseLayer,
@@ -21,6 +25,7 @@ from fairseq.modules import (
     PositionalEmbedding,
     SinusoidalPositionalEmbedding,
 )
+from fairseq.modules.adapter import get_adapter_keys, EfficientAdapter, Adapter
 from fairseq.modules import transformer_layer
 from fairseq.modules.checkpoint_activations import checkpoint_wrapper
 from fairseq.modules.quant_noise import quant_noise as apply_quant_noise_
@@ -55,6 +60,7 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         embed_tokens,
         no_encoder_attn=False,
         output_projection=None,
+        hypernetwork=None,
     ):
         self.cfg = cfg
         super().__init__(dictionary)
@@ -138,6 +144,76 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         if self.output_projection is None:
             self.build_output_projection(cfg, dictionary, embed_tokens)
 
+        # ---------------------------------------------------------------------
+        # Adapters
+        # ---------------------------------------------------------------------
+        if hasattr(cfg, "adapters") and cfg.adapters:
+            self.adapters_lang = cfg.adapters_decoder_lang
+            self.adapters_efficient = cfg.adapters_efficient
+            self.lang_dict = MultilingualDatasetManager.create_lang_dictionary(self.cfg.langs)
+            self.adapters_keys = get_adapter_keys(cfg.lang_pairs, self.adapters_lang)
+
+            # multi-task learning: multiple (task-specific) adapters per layer
+            if self.cfg.adapters_multi:
+                assert self.adapters_lang is not None
+            self.adapters_multi = self.cfg.adapters_multi and len(set(self.adapters_keys)) > 1
+
+            # use separate adapters per language/task
+            if self.adapters_multi:
+                if self.adapters_efficient:
+                    self.adapters = EfficientAdapter(
+                        num_modules=len(self.adapters_keys) * cfg.decoder_layers,
+                        input_size=embed_dim,
+                        bottleneck_size=cfg.adapters_bottle,
+                        activation_fn=cfg.adapters_activation_fn,
+                        static_layernorm=cfg.adapters_static_layernorm,
+                    )
+                else:
+                    self.adapters = nn.ModuleList([])
+                    for i in range(cfg.decoder_layers):
+                        self.adapters.append(
+                            nn.ModuleDict({str(t): Adapter(embed_dim,
+                                                           cfg.adapters_bottle,
+                                                           cfg.adapters_activation_fn,
+                                                           cfg.adapters_static_layernorm)
+                                           for t in self.adapters_keys}))
+
+            # use the same set of adapters for all examples
+            # this is useful for single language/task finetuning
+            else:
+                self.adapters = nn.ModuleList([])
+                for i in range(cfg.decoder_layers):
+                    self.adapters.append(Adapter(embed_dim,
+                                                 cfg.adapters_bottle,
+                                                 cfg.adapters_activation_fn,
+                                                 cfg.adapters_static_layernorm))
+
+        else:
+            self.adapters = None
+
+        # ---------------------------------------------------------------------
+        # Hyper-Adapters
+        # ---------------------------------------------------------------------
+#        if cfg.hyper_adapters:
+#            self.hypernetwork = hypernetwork
+#            self.layer2id = {f"dec-{i}": i for i in range(cfg.decoder_layers)}
+#            self.id2layer = {v: k for k, v in self.layer2id.items()}
+#            self.hyper_adapters_inputs = [int(x in cfg.hyper_adapters_decoder_inputs)
+#                                          for x in ["src", "tgt", "layer"]]
+#        else:
+#            self.hypernetwork = None
+#
+#        # Replacement of (hyper-)adapters language -- for analysis purposes
+#        if cfg.adapters_lang_swap:
+#            logger.info(f"Swapping languages in adapters as follows: {cfg.adapters_lang_swap}")
+#            self.adapter_lang_map = {}
+#            lang_dict = MultilingualDatasetManager.create_lang_dictionary(self.cfg.langs)
+#            for m in cfg.adapters_lang_swap.split(","):
+#                _from, _to = m.split("-")
+#                self.adapter_lang_map[lang_dict.indices[_from]] = lang_dict.indices[_to]
+#        else:
+#            self.adapter_lang_map = None
+
     def build_output_projection(self, cfg, dictionary, embed_tokens):
         if cfg.adaptive_softmax_cutoff is not None:
             self.adaptive_softmax = AdaptiveSoftmax(
@@ -193,6 +269,8 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         alignment_heads: Optional[int] = None,
         src_lengths: Optional[Any] = None,
         return_all_hiddens: bool = False,
+        src_lang_id: Optional[int] = None,
+        tgt_lang_id: Optional[int] = None,
     ):
         """
         Args:
@@ -213,6 +291,15 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
                 - a dictionary with any model-specific outputs
         """
 
+        # replace the language ids used in (hyper-)adapters for analysis purposes
+#        if self.adapter_lang_map is not None:
+#            _src_lang_id = src_lang_id.view(-1)[0].item()
+#            _tgt_lang_id = tgt_lang_id.view(-1)[0].item()
+#            if _src_lang_id in self.adapter_lang_map:
+#                src_lang_id = torch.zeros_like(src_lang_id).fill_(self.adapter_lang_map[_src_lang_id])
+#            if _tgt_lang_id in self.adapter_lang_map:
+#                tgt_lang_id = torch.zeros_like(tgt_lang_id).fill_(self.adapter_lang_map[_tgt_lang_id])
+
         x, extra = self.extract_features(
             prev_output_tokens,
             encoder_out=encoder_out,
@@ -220,6 +307,8 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
             full_context_alignment=full_context_alignment,
             alignment_layer=alignment_layer,
             alignment_heads=alignment_heads,
+            src_lang_id=src_lang_id,
+            tgt_lang_id=tgt_lang_id,
         )
 
         if not features_only:
@@ -234,6 +323,8 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         full_context_alignment: bool = False,
         alignment_layer: Optional[int] = None,
         alignment_heads: Optional[int] = None,
+        src_lang_id: Optional[int] = None,
+        tgt_lang_id: Optional[int] = None,
     ):
         return self.extract_features_scriptable(
             prev_output_tokens,
@@ -242,6 +333,8 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
             full_context_alignment,
             alignment_layer,
             alignment_heads,
+            src_lang_id=src_lang_id,
+            tgt_lang_id=tgt_lang_id,
         )
 
     """
@@ -258,6 +351,8 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         full_context_alignment: bool = False,
         alignment_layer: Optional[int] = None,
         alignment_heads: Optional[int] = None,
+        src_lang_id: Optional[int] = None,
+        tgt_lang_id: Optional[int] = None,
     ):
         """
         Similar to *forward* but only return features.
@@ -347,6 +442,39 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
                 need_attn=bool((idx == alignment_layer)),
                 need_head_weights=bool((idx == alignment_layer)),
             )
+            #  <adapters>
+            if self.adapters is not None:
+                if self.adapters_multi:
+                    # we expect that all src-tgt samples contain the same task/language
+                    if self.adapters_lang == "src":
+                        key = self.lang_dict.symbols[src_lang_id[0]]
+                    elif self.adapters_lang == "tgt":
+                        key = self.lang_dict.symbols[tgt_lang_id[0]]
+                    elif self.adapters_lang == "pair":
+                        src_id = self.lang_dict.symbols[src_lang_id[0]]
+                        tgt_id = self.lang_dict.symbols[tgt_lang_id[0]]
+                        key = f"{src_id}-{tgt_id}"
+
+                    if self.adapters_efficient:
+                        key_id = self.adapters_keys.index(key)
+                        adapter_id = len(self.adapters_keys) * idx + key_id
+                        x = self.adapters(x, adapter_id)
+                    else:
+                        x = self.adapters[idx][key](x)
+                else:
+                    x = self.adapters[idx](x)
+            #  </adapters>
+
+            #  <hyper-adapters>
+#            elif self.hypernetwork is not None:
+#                # note that, src_lang_id and tgt_lang_id start from 1, and
+#                # we assume that all src-tgt samples contain the same task/language
+#                _src_lang_id = src_lang_id[0].squeeze() - 1
+#                _tgt_lang_id = tgt_lang_id[0].squeeze() - 1
+#                layer_id = torch.tensor(self.layer2id[f"dec-{idx}"], device=x.device)
+#                x = self.hypernetwork(x, _src_lang_id, _tgt_lang_id, layer_id,
+#                                      self.hyper_adapters_inputs)
+            #  </hyper-adapters>
             inner_states.append(x)
             if layer_attn is not None and idx == alignment_layer:
                 attn = layer_attn.float().to(x)
diff --git a/fairseq/models/transformer/transformer_encoder.py b/fairseq/models/transformer/transformer_encoder.py
index f007776a..1038e3ce 100644
--- a/fairseq/models/transformer/transformer_encoder.py
+++ b/fairseq/models/transformer/transformer_encoder.py
@@ -9,8 +9,12 @@ from typing import Dict, List, Optional
 import torch
 import torch.nn as nn
 from fairseq import utils
+from fairseq.data.multilingual.multilingual_data_manager import \
+    MultilingualDatasetManager
 from fairseq.distributed import fsdp_wrap
 from fairseq.models import FairseqEncoder
+from fairseq.models.transformer.adapter_helpers import \
+    add_new_layers_to_pretrained
 from fairseq.modules import (
     FairseqDropout,
     LayerDropModuleList,
@@ -18,6 +22,7 @@ from fairseq.modules import (
     PositionalEmbedding,
     SinusoidalPositionalEmbedding,
 )
+from fairseq.modules.adapter import get_adapter_keys, Adapter, EfficientAdapter
 from fairseq.modules import transformer_layer
 from fairseq.modules.checkpoint_activations import checkpoint_wrapper
 from fairseq.modules.quant_noise import quant_noise as apply_quant_noise_
@@ -46,7 +51,7 @@ class TransformerEncoderBase(FairseqEncoder):
         embed_tokens (torch.nn.Embedding): input embedding
     """
 
-    def __init__(self, cfg, dictionary, embed_tokens):
+    def __init__(self, cfg, dictionary, embed_tokens, hypernetwork=None):
         self.cfg = cfg
         super().__init__(dictionary)
         self.register_buffer("version", torch.Tensor([3]))
@@ -102,6 +107,77 @@ class TransformerEncoderBase(FairseqEncoder):
         else:
             self.layer_norm = None
 
+        # ---------------------------------------------------------------------
+        # Adapters
+        # ---------------------------------------------------------------------
+        if hasattr(cfg, "adapters") and cfg.adapters:
+            self.adapters_lang = cfg.adapters_encoder_lang
+            self.adapters_efficient = cfg.adapters_efficient
+            self.lang_dict = MultilingualDatasetManager.create_lang_dictionary(self.cfg.langs)
+            self.adapters_keys = get_adapter_keys(cfg.lang_pairs, self.adapters_lang)
+
+            # multi-task learning: multiple (task-specific) adapters per layer
+            if self.cfg.adapters_multi:
+                assert self.adapters_lang is not None
+            self.adapters_multi = self.cfg.adapters_multi and len(set(self.adapters_keys)) > 1
+
+            # use separate adapters per language/task
+            if self.adapters_multi:
+                if self.adapters_efficient:
+                    self.adapters = EfficientAdapter(
+                        num_modules=len(self.adapters_keys) * cfg.encoder_layers,
+                        input_size=embed_dim,
+                        bottleneck_size=cfg.adapters_bottle,
+                        activation_fn=cfg.adapters_activation_fn,
+                        static_layernorm=cfg.adapters_static_layernorm,
+                    )
+                else:
+                    self.adapters = nn.ModuleList([])
+                    for i in range(cfg.encoder_layers):
+                        self.adapters.append(
+                            nn.ModuleDict({str(t): Adapter(embed_dim,
+                                                           cfg.adapters_bottle,
+                                                           cfg.adapters_activation_fn,
+                                                           cfg.adapters_static_layernorm)
+                                           for t in self.adapters_keys}))
+
+            # use the same set of adapters for all examples
+            # this is useful for single language/task finetuning
+            else:
+                self.adapters = nn.ModuleList([])
+                for i in range(cfg.encoder_layers):
+                    self.adapters.append(Adapter(embed_dim,
+                                                 cfg.adapters_bottle,
+                                                 cfg.adapters_activation_fn,
+                                                 cfg.adapters_static_layernorm))
+
+        else:
+            self.adapters = None
+
+        # ---------------------------------------------------------------------
+        # Hyper-Adapters
+        # ---------------------------------------------------------------------
+#        if cfg.hyper_adapters:
+#            self.hypernetwork = hypernetwork
+#            self.layer2id = {f"enc-{i}": i for i in range(cfg.encoder_layers)}
+#            self.id2layer = {v: k for k, v in self.layer2id.items()}
+#            self.hyper_adapters_inputs = [int(x in cfg.hyper_adapters_encoder_inputs)
+#                                          for x in ["src", "tgt", "layer"]]
+#        else:
+#            self.hypernetwork = None
+#
+#        # Replacement of (hyper-)adapters language -- for analysis purposes
+#        if cfg.adapters_lang_swap:
+#            logger.info(f"Swapping languages in adapters as follows: {cfg.adapters_lang_swap}")
+#            self.adapter_lang_map = {}
+#            lang_dict = MultilingualDatasetManager.create_lang_dictionary(self.cfg.langs)
+#            for m in cfg.adapters_lang_swap.split(","):
+#                _from, _to = m.split("-")
+#                self.adapter_lang_map[lang_dict.indices[_from]] = lang_dict.indices[_to]
+#        else:
+#            self.adapter_lang_map = None
+
+
     def build_encoder_layer(self, cfg):
         layer = transformer_layer.TransformerEncoderLayerBase(cfg)
         checkpoint = cfg.checkpoint_activations
@@ -136,6 +212,8 @@ class TransformerEncoderBase(FairseqEncoder):
         src_lengths: Optional[torch.Tensor] = None,
         return_all_hiddens: bool = False,
         token_embeddings: Optional[torch.Tensor] = None,
+        src_lang_id: Optional[int] = None,
+        tgt_lang_id: Optional[int] = None,
     ):
         """
         Args:
@@ -160,8 +238,19 @@ class TransformerEncoderBase(FairseqEncoder):
                   hidden states of shape `(src_len, batch, embed_dim)`.
                   Only populated if *return_all_hiddens* is True.
         """
+
+#        # replace the language ids used in (hyper-)adapters for analysis purposes
+#        if self.adapter_lang_map is not None:
+#            _src_lang_id = src_lang_id.view(-1)[0].item()
+#            _tgt_lang_id = tgt_lang_id.view(-1)[0].item()
+#            if _src_lang_id in self.adapter_lang_map:
+#                src_lang_id = torch.zeros_like(src_lang_id).fill_(self.adapter_lang_map[_src_lang_id])
+#            if _tgt_lang_id in self.adapter_lang_map:
+#                tgt_lang_id = torch.zeros_like(tgt_lang_id).fill_(self.adapter_lang_map[_tgt_lang_id])
+#
         return self.forward_scriptable(
-            src_tokens, src_lengths, return_all_hiddens, token_embeddings
+            src_tokens, src_lengths, return_all_hiddens, token_embeddings,
+            src_lang_id=src_lang_id, tgt_lang_id=tgt_lang_id
         )
 
     # TorchScript doesn't support super() method so that the scriptable Subclass
@@ -174,6 +263,8 @@ class TransformerEncoderBase(FairseqEncoder):
         src_lengths: Optional[torch.Tensor] = None,
         return_all_hiddens: bool = False,
         token_embeddings: Optional[torch.Tensor] = None,
+        src_lang_id: Optional[int] = None,
+        tgt_lang_id: Optional[int] = None,
     ):
         """
         Args:
@@ -217,13 +308,46 @@ class TransformerEncoderBase(FairseqEncoder):
             encoder_states.append(x)
 
         # encoder layers
-        for layer in self.layers:
+        for idx, layer in enumerate(self.layers):
             x = layer(
                 x, encoder_padding_mask=encoder_padding_mask if has_pads else None
             )
             if return_all_hiddens:
                 assert encoder_states is not None
                 encoder_states.append(x)
+            #  <adapters>
+            if self.adapters is not None:
+                if self.adapters_multi:
+                    # we expect all src-tgt samples contain the same task/language
+                    if self.adapters_lang == "src":
+                        key = self.lang_dict.symbols[src_lang_id[0]]
+                    elif self.adapters_lang == "tgt":
+                        key = self.lang_dict.symbols[tgt_lang_id[0]]
+                    elif self.adapters_lang == "pair":
+                        src_id = self.lang_dict.symbols[src_lang_id[0]]
+                        tgt_id = self.lang_dict.symbols[tgt_lang_id[0]]
+                        key = f"{src_id}-{tgt_id}"
+
+                    if self.adapters_efficient:
+                        key_id = self.adapters_keys.index(key)
+                        adapter_id = len(self.adapters_keys) * idx + key_id
+                        x = self.adapters(x, adapter_id)
+                    else:
+                        x = self.adapters[idx][key](x)
+                else:
+                    x = self.adapters[idx](x)
+            #  </adapters>
+
+            #  <hyper-adapters>
+            #elif self.hypernetwork is not None:
+            #    # note that, src_lang_id and tgt_lang_id start from 1, and
+            #    # we assume all src-tgt samples contain the same task/language
+            #    _src_lang_id = src_lang_id[0].squeeze() - 1
+            #    _tgt_lang_id = tgt_lang_id[0].squeeze() - 1
+            #    layer_id = torch.tensor(self.layer2id[f"enc-{idx}"], device=x.device)
+            #    x = self.hypernetwork(x, _src_lang_id, _tgt_lang_id, layer_id,
+            #                          self.hyper_adapters_inputs)
+            #  </hyper-adapters>
 
         if self.layer_norm is not None:
             x = self.layer_norm(x)
diff --git a/fairseq/options.py b/fairseq/options.py
index 03883fc5..5832d31d 100644
--- a/fairseq/options.py
+++ b/fairseq/options.py
@@ -56,6 +56,29 @@ def get_generation_parser(interactive=False, default_task="translation"):
     return parser
 
 
+def get_experimental_generation_parser(interactive=False, default_task="translation"):
+    parser = get_parser("Generation", default_task)
+    add_dataset_args(parser, gen=True)
+    add_distributed_training_args(parser, default_world_size=1)
+    add_model_args(parser)
+    add_generation_args(parser)
+    add_knn_record_args(parser)
+    if interactive:
+        add_interactive_args(parser)
+    return parser
+
+
+def get_knn_generation_parser(interactive=False, default_task="translation"):
+    parser = get_parser("Generation", default_task)
+    add_dataset_args(parser, gen=True)
+    add_distributed_training_args(parser, default_world_size=1)
+    add_generation_args(parser)
+    add_datastore_args(parser)
+    if interactive:
+        add_interactive_args(parser)
+    return parser
+
+
 def get_interactive_generation_parser(default_task="translation"):
     return get_generation_parser(interactive=True, default_task=default_task)
 
@@ -77,6 +100,19 @@ def get_validation_parser(default_task=None):
     return parser
 
 
+def get_save_datastore_parser(default_task=None):
+    # add by zx, just copy from get_validation_parser
+    # and add datastore args
+
+    parser = get_parser("Validation", default_task)
+    add_dataset_args(parser, train=True)
+    add_distributed_training_args(parser, default_world_size=1)
+    add_datastore_args(parser)
+    group = parser.add_argument_group("Evaluation")
+    gen_parser_from_dataclass(group, CommonEvalConfig())
+    return parser
+
+
 def parse_args_and_arch(
     parser: argparse.ArgumentParser,
     input_args: List[str] = None,
@@ -294,6 +330,47 @@ def add_preprocess_args(parser):
     return parser
 
 
+def add_datastore_args(parser):
+    group = parser.add_argument_group("datastore")
+    group.add_argument("--dstore-fp16", action='store_true',
+                       help="if save only fp16")
+    group.add_argument("--dstore-size", metavar="N", default=1, type=int,
+                       help="datastore size")
+    group.add_argument("--dstore-mmap", default=None, type=str, help="save dir for datastore")
+    group.add_argument("--decoder-embed-dim", metavar="N", default=1024, type=int,
+                       help="decoder embedding size")
+    group.add_argument("--multidomain-shuffle", default=False, action='store_true')
+    group.add_argument("--use-knn-store", default=False, action='store_true')
+    group.add_argument("--k", default=16, type=int)
+    group.add_argument("--knn-coefficient", default=0, type=float, help="this has been duplicated")
+    group.add_argument("--faiss-metric-type", default=None, type=str)
+    group.add_argument("--knn-sim-func", default=None, type=str)
+    group.add_argument("--knn-temperature", default=1., type=float)
+    group.add_argument("--use-gpu-to-search", default=False, action='store_true')
+    group.add_argument("--dstore-filename", default=None, type=str)
+    group.add_argument("--move-dstore-to-mem", default=False, action='store_true')
+    group.add_argument("--indexfile", default=None, type=str)
+    group.add_argument("--probe", default=8, type=int)
+    group.add_argument("--no-load-keys", default=False, action='store_true')
+    group.add_argument("--only-use-max-idx", default=False, action='store_true')
+    group.add_argument("--save-plain-text", default=False, action='store_true')
+    group.add_argument("--plain-text-file", default=None, type=str)
+
+    # for deciding knn lambda value
+    group.add_argument("--lambda-type", default=None, type=str, help="fix, based_on_step, based_on_distance")
+    group.add_argument("--lambda-value", default=0, type=float, help="used when lambda type is fix")
+    group.add_argument("--min-lambda-value", default=0, type=float, help="")
+    group.add_argument("--max-lambda-value", default=0, type=float, help="")
+    group.add_argument("--knn-step-bound", default=0, type=int, help="")
+    group.add_argument("--lambda-tend", default=None, type=str, help="increase or decrease")
+    group.add_argument("--lambda-curve", default='linear', type=str, help="")
+
+    # for check knn distance and idx
+    group.add_argument("--check-knn-result", default=False, action='store_true')
+
+    return parser
+
+
 def add_dataset_args(parser, train=False, gen=False):
     group = parser.add_argument_group("dataset_data_loading")
     gen_parser_from_dataclass(group, DatasetConfig())
@@ -368,6 +445,15 @@ def add_model_args(parser):
     return group
 
 
+def add_knn_record_args(parser):
+    group = parser.add_argument_group("kNN Record")
+    # fmt: off
+    group.add_argument('--knn-record-index', default=False, action='store_true')
+    group.add_argument('--knn-record-distance', default=False, action='store_true')
+    group.add_argument('--knn-record-lambda', default=False, action='store_true')
+    group.add_argument('--knn-record-label-counts', default=False, action='store_true')
+
+
 def get_args(
     data: Union[str, Path],
     task: str = "translation",
diff --git a/fairseq/sequence_generator.py b/fairseq/sequence_generator.py
deleted file mode 100644
index 2e61140d..00000000
--- a/fairseq/sequence_generator.py
+++ /dev/null
@@ -1,973 +0,0 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-import math
-from typing import Dict, List, Optional
-import sys
-
-import torch
-import torch.nn as nn
-from fairseq import search, utils
-from fairseq.data import data_utils
-from fairseq.models import FairseqIncrementalDecoder
-from torch import Tensor
-from fairseq.ngram_repeat_block import NGramRepeatBlock
-
-
-class SequenceGenerator(nn.Module):
-    def __init__(
-        self,
-        models,
-        tgt_dict,
-        beam_size=1,
-        max_len_a=0,
-        max_len_b=200,
-        max_len=0,
-        min_len=1,
-        normalize_scores=True,
-        len_penalty=1.0,
-        unk_penalty=0.0,
-        temperature=1.0,
-        match_source_len=False,
-        no_repeat_ngram_size=0,
-        search_strategy=None,
-        eos=None,
-        symbols_to_strip_from_output=None,
-        lm_model=None,
-        lm_weight=1.0,
-    ):
-        """Generates translations of a given source sentence.
-
-        Args:
-            models (List[~fairseq.models.FairseqModel]): ensemble of models,
-                currently support fairseq.models.TransformerModel for scripting
-            beam_size (int, optional): beam width (default: 1)
-            max_len_a/b (int, optional): generate sequences of maximum length
-                ax + b, where x is the source length
-            max_len (int, optional): the maximum length of the generated output
-                (not including end-of-sentence)
-            min_len (int, optional): the minimum length of the generated output
-                (not including end-of-sentence)
-            normalize_scores (bool, optional): normalize scores by the length
-                of the output (default: True)
-            len_penalty (float, optional): length penalty, where <1.0 favors
-                shorter, >1.0 favors longer sentences (default: 1.0)
-            unk_penalty (float, optional): unknown word penalty, where <0
-                produces more unks, >0 produces fewer (default: 0.0)
-            temperature (float, optional): temperature, where values
-                >1.0 produce more uniform samples and values <1.0 produce
-                sharper samples (default: 1.0)
-            match_source_len (bool, optional): outputs should match the source
-                length (default: False)
-        """
-        super().__init__()
-        if isinstance(models, EnsembleModel):
-            self.model = models
-        else:
-            self.model = EnsembleModel(models)
-        self.tgt_dict = tgt_dict
-        self.pad = tgt_dict.pad()
-        self.unk = tgt_dict.unk()
-        self.eos = tgt_dict.eos() if eos is None else eos
-        self.symbols_to_strip_from_output = (
-            symbols_to_strip_from_output.union({self.eos})
-            if symbols_to_strip_from_output is not None
-            else {self.eos}
-        )
-        self.vocab_size = len(tgt_dict)
-        self.beam_size = beam_size
-        # the max beam size is the dictionary size - 1, since we never select pad
-        self.beam_size = min(beam_size, self.vocab_size - 1)
-        self.max_len_a = max_len_a
-        self.max_len_b = max_len_b
-        self.min_len = min_len
-        self.max_len = max_len or self.model.max_decoder_positions()
-
-        self.normalize_scores = normalize_scores
-        self.len_penalty = len_penalty
-        self.unk_penalty = unk_penalty
-        self.temperature = temperature
-        self.match_source_len = match_source_len
-
-        if no_repeat_ngram_size > 0:
-            self.repeat_ngram_blocker = NGramRepeatBlock(no_repeat_ngram_size)
-        else:
-            self.repeat_ngram_blocker = None
-
-        assert temperature > 0, "--temperature must be greater than 0"
-
-        self.search = (
-            search.BeamSearch(tgt_dict) if search_strategy is None else search_strategy
-        )
-        # We only need to set src_lengths in LengthConstrainedBeamSearch.
-        # As a module attribute, setting it would break in multithread
-        # settings when the model is shared.
-        self.should_set_src_lengths = (
-            hasattr(self.search, "needs_src_lengths") and self.search.needs_src_lengths
-        )
-
-        self.model.eval()
-
-        self.lm_model = lm_model
-        self.lm_weight = lm_weight
-        if self.lm_model is not None:
-            self.lm_model.eval()
-
-    def cuda(self):
-        self.model.cuda()
-        return self
-
-    @torch.no_grad()
-    def forward(
-        self,
-        sample: Dict[str, Dict[str, Tensor]],
-        prefix_tokens: Optional[Tensor] = None,
-        bos_token: Optional[int] = None,
-    ):
-        """Generate a batch of translations.
-
-        Args:
-            sample (dict): batch
-            prefix_tokens (torch.LongTensor, optional): force decoder to begin
-                with these tokens
-            bos_token (int, optional): beginning of sentence token
-                (default: self.eos)
-        """
-        return self._generate(sample, prefix_tokens, bos_token=bos_token)
-
-    # TODO(myleott): unused, deprecate after pytorch-translate migration
-    def generate_batched_itr(self, data_itr, beam_size=None, cuda=False, timer=None):
-        """Iterate over a batched dataset and yield individual translations.
-        Args:
-            cuda (bool, optional): use GPU for generation
-            timer (StopwatchMeter, optional): time generations
-        """
-        for sample in data_itr:
-            s = utils.move_to_cuda(sample) if cuda else sample
-            if "net_input" not in s:
-                continue
-            input = s["net_input"]
-            # model.forward normally channels prev_output_tokens into the decoder
-            # separately, but SequenceGenerator directly calls model.encoder
-            encoder_input = {
-                k: v for k, v in input.items() if k != "prev_output_tokens"
-            }
-            if timer is not None:
-                timer.start()
-            with torch.no_grad():
-                hypos = self.generate(encoder_input)
-            if timer is not None:
-                timer.stop(sum(len(h[0]["tokens"]) for h in hypos))
-            for i, id in enumerate(s["id"].data):
-                # remove padding
-                src = utils.strip_pad(input["src_tokens"].data[i, :], self.pad)
-                ref = (
-                    utils.strip_pad(s["target"].data[i, :], self.pad)
-                    if s["target"] is not None
-                    else None
-                )
-                yield id, src, ref, hypos[i]
-
-    @torch.no_grad()
-    def generate(self, models, sample: Dict[str, Dict[str, Tensor]], **kwargs) -> List[List[Dict[str, Tensor]]]:
-        """Generate translations. Match the api of other fairseq generators.
-
-        Args:
-            models (List[~fairseq.models.FairseqModel]): ensemble of models
-            sample (dict): batch
-            prefix_tokens (torch.LongTensor, optional): force decoder to begin
-                with these tokens
-            constraints (torch.LongTensor, optional): force decoder to include
-                the list of constraints
-            bos_token (int, optional): beginning of sentence token
-                (default: self.eos)
-        """
-        return self._generate(sample, **kwargs)
-
-    def _generate(
-        self,
-        sample: Dict[str, Dict[str, Tensor]],
-        prefix_tokens: Optional[Tensor] = None,
-        constraints: Optional[Tensor] = None,
-        bos_token: Optional[int] = None,
-    ):
-        incremental_states = torch.jit.annotate(
-            List[Dict[str, Dict[str, Optional[Tensor]]]],
-            [
-                torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {})
-                for i in range(self.model.models_size)
-            ],
-        )
-        net_input = sample["net_input"]
-
-        if "src_tokens" in net_input:
-            src_tokens = net_input["src_tokens"]
-            # length of the source text being the character length except EndOfSentence and pad
-            src_lengths = (
-                (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)
-            )
-        elif "source" in net_input:
-            src_tokens = net_input["source"]
-            src_lengths = (
-                net_input["padding_mask"].size(-1) - net_input["padding_mask"].sum(-1)
-                if net_input["padding_mask"] is not None
-                else torch.tensor(src_tokens.size(-1)).to(src_tokens)
-            )
-        elif "features" in net_input:
-            src_tokens = net_input["features"]
-            src_lengths = (
-                net_input["padding_mask"].size(-1) - net_input["padding_mask"].sum(-1)
-                if net_input["padding_mask"] is not None
-                else torch.tensor(src_tokens.size(-1)).to(src_tokens)
-            )
-        else:
-            raise Exception("expected src_tokens or source in net input. input keys: " + str(net_input.keys()))
-
-        # bsz: total number of sentences in beam
-        # Note that src_tokens may have more than 2 dimensions (i.e. audio features)
-        bsz, src_len = src_tokens.size()[:2]
-        beam_size = self.beam_size
-
-        if constraints is not None and not self.search.supports_constraints:
-            raise NotImplementedError(
-                "Target-side constraints were provided, but search method doesn't support them"
-            )
-
-        # Initialize constraints, when active
-        self.search.init_constraints(constraints, beam_size)
-
-        max_len: int = -1
-        if self.match_source_len:
-            max_len = src_lengths.max().item()
-        else:
-            max_len = min(
-                int(self.max_len_a * src_len + self.max_len_b),
-                self.max_len - 1,
-            )
-        assert (
-            self.min_len <= max_len
-        ), "min_len cannot be larger than max_len, please adjust these!"
-        # compute the encoder output for each beam
-        with torch.autograd.profiler.record_function("EnsembleModel: forward_encoder"):
-            encoder_outs = self.model.forward_encoder(net_input)
-
-        # placeholder of indices for bsz * beam_size to hold tokens and accumulative scores
-        new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)
-        new_order = new_order.to(src_tokens.device).long()
-        encoder_outs = self.model.reorder_encoder_out(encoder_outs, new_order)
-        # ensure encoder_outs is a List.
-        assert encoder_outs is not None
-
-        # initialize buffers
-        scores = (
-            torch.zeros(bsz * beam_size, max_len + 1).to(src_tokens).float()
-        )  # +1 for eos; pad is never chosen for scoring
-        tokens = (
-            torch.zeros(bsz * beam_size, max_len + 2)
-            .to(src_tokens)
-            .long()
-            .fill_(self.pad)
-        )  # +2 for eos and pad
-        tokens[:, 0] = self.eos if bos_token is None else bos_token
-        attn: Optional[Tensor] = None
-
-        # A list that indicates candidates that should be ignored.
-        # For example, suppose we're sampling and have already finalized 2/5
-        # samples. Then cands_to_ignore would mark 2 positions as being ignored,
-        # so that we only finalize the remaining 3 samples.
-        cands_to_ignore = (
-            torch.zeros(bsz, beam_size).to(src_tokens).eq(-1)
-        )  # forward and backward-compatible False mask
-
-        # list of completed sentences
-        finalized = torch.jit.annotate(
-            List[List[Dict[str, Tensor]]],
-            [torch.jit.annotate(List[Dict[str, Tensor]], []) for i in range(bsz)],
-        )  # contains lists of dictionaries of infomation about the hypothesis being finalized at each step
-
-        # a boolean array indicating if the sentence at the index is finished or not
-        finished = [False for i in range(bsz)]
-        num_remaining_sent = bsz  # number of sentences remaining
-
-        # number of candidate hypos per step
-        cand_size = 2 * beam_size  # 2 x beam size in case half are EOS
-
-        # offset arrays for converting between different indexing schemes
-        bbsz_offsets = (
-            (torch.arange(0, bsz) * beam_size)
-            .unsqueeze(1)
-            .type_as(tokens)
-            .to(src_tokens.device)
-        )
-        cand_offsets = torch.arange(0, cand_size).type_as(tokens).to(src_tokens.device)
-
-        reorder_state: Optional[Tensor] = None
-        batch_idxs: Optional[Tensor] = None
-
-        original_batch_idxs: Optional[Tensor] = None
-        if "id" in sample and isinstance(sample["id"], Tensor):
-            original_batch_idxs = sample["id"]
-        else:
-            original_batch_idxs = torch.arange(0, bsz).type_as(tokens)
-
-        for step in range(max_len + 1):  # one extra step for EOS marker
-            # reorder decoder internal states based on the prev choice of beams
-            if reorder_state is not None:
-                if batch_idxs is not None:
-                    # update beam indices to take into account removed sentences
-                    corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(
-                        batch_idxs
-                    )
-                    reorder_state.view(-1, beam_size).add_(
-                        corr.unsqueeze(-1) * beam_size
-                    )
-                    original_batch_idxs = original_batch_idxs[batch_idxs]
-                self.model.reorder_incremental_state(incremental_states, reorder_state)
-                encoder_outs = self.model.reorder_encoder_out(
-                    encoder_outs, reorder_state
-                )
-            with torch.autograd.profiler.record_function("EnsembleModel: forward_decoder"):
-                lprobs, avg_attn_scores = self.model.forward_decoder(
-                    tokens[:, : step + 1],
-                    encoder_outs,
-                    incremental_states,
-                    self.temperature,
-                )
-
-            if self.lm_model is not None:
-                lm_out = self.lm_model(tokens[:, : step + 1])
-                probs = self.lm_model.get_normalized_probs(
-                    lm_out, log_probs=True, sample=None
-                )
-                probs = probs[:, -1, :] * self.lm_weight
-                lprobs += probs
-            # handle prefix tokens (possibly with different lengths)
-            if (
-                prefix_tokens is not None
-                and step < prefix_tokens.size(1)
-                and step < max_len
-            ):
-                lprobs, tokens, scores = self._prefix_tokens(
-                    step, lprobs, scores, tokens, prefix_tokens, beam_size
-                )
-            elif step < self.min_len:
-                # minimum length constraint (does not apply if using prefix_tokens)
-                lprobs[:, self.eos] = -math.inf
-
-            lprobs[lprobs != lprobs] = torch.tensor(-math.inf).to(lprobs)
-
-            lprobs[:, self.pad] = -math.inf  # never select pad
-            lprobs[:, self.unk] -= self.unk_penalty  # apply unk penalty
-
-            # handle max length constraint
-            if step >= max_len:
-                lprobs[:, : self.eos] = -math.inf
-                lprobs[:, self.eos + 1 :] = -math.inf
-
-            # Record attention scores, only support avg_attn_scores is a Tensor
-            if avg_attn_scores is not None:
-                if attn is None:
-                    attn = torch.empty(
-                        bsz * beam_size, avg_attn_scores.size(1), max_len + 2
-                    ).to(scores)
-                attn[:, :, step + 1].copy_(avg_attn_scores)
-
-            scores = scores.type_as(lprobs)
-            eos_bbsz_idx = torch.empty(0).to(
-                tokens
-            )  # indices of hypothesis ending with eos (finished sentences)
-            eos_scores = torch.empty(0).to(
-                scores
-            )  # scores of hypothesis ending with eos (finished sentences)
-
-            if self.should_set_src_lengths:
-                self.search.set_src_lengths(src_lengths)
-
-            if self.repeat_ngram_blocker is not None:
-                lprobs = self.repeat_ngram_blocker(tokens, lprobs, bsz, beam_size, step)
-
-            # Shape: (batch, cand_size)
-            cand_scores, cand_indices, cand_beams = self.search.step(
-                step,
-                lprobs.view(bsz, -1, self.vocab_size),
-                scores.view(bsz, beam_size, -1)[:, :, :step],
-                tokens[:, : step + 1],
-                original_batch_idxs,
-            )
-
-            # cand_bbsz_idx contains beam indices for the top candidate
-            # hypotheses, with a range of values: [0, bsz*beam_size),
-            # and dimensions: [bsz, cand_size]
-            cand_bbsz_idx = cand_beams.add(bbsz_offsets)
-
-            # finalize hypotheses that end in eos
-            # Shape of eos_mask: (batch size, beam size)
-            eos_mask = cand_indices.eq(self.eos) & cand_scores.ne(-math.inf)
-            eos_mask[:, :beam_size][cands_to_ignore] = torch.tensor(0).to(eos_mask)
-
-            # only consider eos when it's among the top beam_size indices
-            # Now we know what beam item(s) to finish
-            # Shape: 1d list of absolute-numbered
-            eos_bbsz_idx = torch.masked_select(
-                cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size]
-            )
-
-            finalized_sents: List[int] = []
-            if eos_bbsz_idx.numel() > 0:
-                eos_scores = torch.masked_select(
-                    cand_scores[:, :beam_size], mask=eos_mask[:, :beam_size]
-                )
-
-                finalized_sents = self.finalize_hypos(
-                    step,
-                    eos_bbsz_idx,
-                    eos_scores,
-                    tokens,
-                    scores,
-                    finalized,
-                    finished,
-                    beam_size,
-                    attn,
-                    src_lengths,
-                    max_len,
-                )
-                num_remaining_sent -= len(finalized_sents)
-
-            assert num_remaining_sent >= 0
-            if num_remaining_sent == 0:
-                break
-            if self.search.stop_on_max_len and step >= max_len:
-                break
-            assert step < max_len, f"{step} < {max_len}"
-
-            # Remove finalized sentences (ones for which {beam_size}
-            # finished hypotheses have been generated) from the batch.
-            if len(finalized_sents) > 0:
-                new_bsz = bsz - len(finalized_sents)
-
-                # construct batch_idxs which holds indices of batches to keep for the next pass
-                batch_mask = torch.ones(
-                    bsz, dtype=torch.bool, device=cand_indices.device
-                )
-                batch_mask[finalized_sents] = False
-                # TODO replace `nonzero(as_tuple=False)` after TorchScript supports it
-                batch_idxs = torch.arange(
-                    bsz, device=cand_indices.device
-                ).masked_select(batch_mask)
-
-                # Choose the subset of the hypothesized constraints that will continue
-                self.search.prune_sentences(batch_idxs)
-
-                eos_mask = eos_mask[batch_idxs]
-                cand_beams = cand_beams[batch_idxs]
-                bbsz_offsets.resize_(new_bsz, 1)
-                cand_bbsz_idx = cand_beams.add(bbsz_offsets)
-                cand_scores = cand_scores[batch_idxs]
-                cand_indices = cand_indices[batch_idxs]
-
-                if prefix_tokens is not None:
-                    prefix_tokens = prefix_tokens[batch_idxs]
-                src_lengths = src_lengths[batch_idxs]
-                cands_to_ignore = cands_to_ignore[batch_idxs]
-
-                scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)
-                tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)
-                if attn is not None:
-                    attn = attn.view(bsz, -1)[batch_idxs].view(
-                        new_bsz * beam_size, attn.size(1), -1
-                    )
-                bsz = new_bsz
-            else:
-                batch_idxs = None
-
-            # Set active_mask so that values > cand_size indicate eos hypos
-            # and values < cand_size indicate candidate active hypos.
-            # After, the min values per row are the top candidate active hypos
-
-            # Rewrite the operator since the element wise or is not supported in torchscript.
-
-            eos_mask[:, :beam_size] = ~((~cands_to_ignore) & (~eos_mask[:, :beam_size]))
-            active_mask = torch.add(
-                eos_mask.type_as(cand_offsets) * cand_size,
-                cand_offsets[: eos_mask.size(1)],
-            )
-
-            # get the top beam_size active hypotheses, which are just
-            # the hypos with the smallest values in active_mask.
-            # {active_hypos} indicates which {beam_size} hypotheses
-            # from the list of {2 * beam_size} candidates were
-            # selected. Shapes: (batch size, beam size)
-            new_cands_to_ignore, active_hypos = torch.topk(
-                active_mask, k=beam_size, dim=1, largest=False
-            )
-
-            # update cands_to_ignore to ignore any finalized hypos.
-            cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]
-            # Make sure there is at least one active item for each sentence in the batch.
-            assert (~cands_to_ignore).any(dim=1).all()
-
-            # update cands_to_ignore to ignore any finalized hypos
-
-            # {active_bbsz_idx} denotes which beam number is continued for each new hypothesis (a beam
-            # can be selected more than once).
-            active_bbsz_idx = torch.gather(cand_bbsz_idx, dim=1, index=active_hypos)
-            active_scores = torch.gather(cand_scores, dim=1, index=active_hypos)
-
-            active_bbsz_idx = active_bbsz_idx.view(-1)
-            active_scores = active_scores.view(-1)
-
-            # copy tokens and scores for active hypotheses
-
-            # Set the tokens for each beam (can select the same row more than once)
-            tokens[:, : step + 1] = torch.index_select(
-                tokens[:, : step + 1], dim=0, index=active_bbsz_idx
-            )
-            # Select the next token for each of them
-            tokens.view(bsz, beam_size, -1)[:, :, step + 1] = torch.gather(
-                cand_indices, dim=1, index=active_hypos
-            )
-            if step > 0:
-                scores[:, :step] = torch.index_select(
-                    scores[:, :step], dim=0, index=active_bbsz_idx
-                )
-            scores.view(bsz, beam_size, -1)[:, :, step] = torch.gather(
-                cand_scores, dim=1, index=active_hypos
-            )
-
-            # Update constraints based on which candidates were selected for the next beam
-            self.search.update_constraints(active_hypos)
-
-            # copy attention for active hypotheses
-            if attn is not None:
-                attn[:, :, : step + 2] = torch.index_select(
-                    attn[:, :, : step + 2], dim=0, index=active_bbsz_idx
-                )
-
-            # reorder incremental state in decoder
-            reorder_state = active_bbsz_idx
-
-        # sort by score descending
-        for sent in range(len(finalized)):
-            scores = torch.tensor(
-                [float(elem["score"].item()) for elem in finalized[sent]]
-            )
-            _, sorted_scores_indices = torch.sort(scores, descending=True)
-            finalized[sent] = [finalized[sent][ssi] for ssi in sorted_scores_indices]
-            finalized[sent] = torch.jit.annotate(
-                List[Dict[str, Tensor]], finalized[sent]
-            )
-        return finalized
-
-    def _prefix_tokens(
-        self, step: int, lprobs, scores, tokens, prefix_tokens, beam_size: int
-    ):
-        """Handle prefix tokens"""
-        prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)
-        prefix_lprobs = lprobs.gather(-1, prefix_toks.unsqueeze(-1))
-        prefix_mask = prefix_toks.ne(self.pad)
-        lprobs[prefix_mask] = torch.min(prefix_lprobs) - 1
-        lprobs[prefix_mask] = lprobs[prefix_mask].scatter(
-            -1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lprobs[prefix_mask]
-        )
-        # if prefix includes eos, then we should make sure tokens and
-        # scores are the same across all beams
-        eos_mask = prefix_toks.eq(self.eos)
-        if eos_mask.any():
-            # validate that the first beam matches the prefix
-            first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[
-                :, 0, 1 : step + 1
-            ]
-            eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]
-            target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]
-            assert (first_beam == target_prefix).all()
-
-            # copy tokens, scores and lprobs from the first beam to all beams
-            tokens = self.replicate_first_beam(tokens, eos_mask_batch_dim, beam_size)
-            scores = self.replicate_first_beam(scores, eos_mask_batch_dim, beam_size)
-            lprobs = self.replicate_first_beam(lprobs, eos_mask_batch_dim, beam_size)
-        return lprobs, tokens, scores
-
-    def replicate_first_beam(self, tensor, mask, beam_size: int):
-        tensor = tensor.view(-1, beam_size, tensor.size(-1))
-        tensor[mask] = tensor[mask][:, :1, :]
-        return tensor.view(-1, tensor.size(-1))
-
-    def finalize_hypos(
-        self,
-        step: int,
-        bbsz_idx,
-        eos_scores,
-        tokens,
-        scores,
-        finalized: List[List[Dict[str, Tensor]]],
-        finished: List[bool],
-        beam_size: int,
-        attn: Optional[Tensor],
-        src_lengths,
-        max_len: int,
-    ):
-        """Finalize hypothesis, store finalized information in `finalized`, and change `finished` accordingly.
-        A sentence is finalized when {beam_size} finished items have been collected for it.
-
-        Returns number of sentences (not beam items) being finalized.
-        These will be removed from the batch and not processed further.
-        Args:
-            bbsz_idx (Tensor):
-        """
-        assert bbsz_idx.numel() == eos_scores.numel()
-
-        # clone relevant token and attention tensors.
-        # tokens is (batch * beam, max_len). So the index_select
-        # gets the newly EOS rows, then selects cols 1..{step + 2}
-        tokens_clone = tokens.index_select(0, bbsz_idx)[
-            :, 1 : step + 2
-        ]  # skip the first index, which is EOS
-
-        tokens_clone[:, step] = self.eos
-        attn_clone = (
-            attn.index_select(0, bbsz_idx)[:, :, 1 : step + 2]
-            if attn is not None
-            else None
-        )
-
-        # compute scores per token position
-        pos_scores = scores.index_select(0, bbsz_idx)[:, : step + 1]
-        pos_scores[:, step] = eos_scores
-        # convert from cumulative to per-position scores
-        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]
-
-        # normalize sentence-level scores
-        if self.normalize_scores:
-            eos_scores /= (step + 1) ** self.len_penalty
-
-        # cum_unfin records which sentences in the batch are finished.
-        # It helps match indexing between (a) the original sentences
-        # in the batch and (b) the current, possibly-reduced set of
-        # sentences.
-        cum_unfin: List[int] = []
-        prev = 0
-        for f in finished:
-            if f:
-                prev += 1
-            else:
-                cum_unfin.append(prev)
-        cum_fin_tensor = torch.tensor(cum_unfin, dtype=torch.int).to(bbsz_idx)
-
-        unfin_idx = bbsz_idx // beam_size
-        sent = unfin_idx + torch.index_select(cum_fin_tensor, 0, unfin_idx)
-
-        # Create a set of "{sent}{unfin_idx}", where
-        # "unfin_idx" is the index in the current (possibly reduced)
-        # list of sentences, and "sent" is the index in the original,
-        # unreduced batch
-        # For every finished beam item
-        # sentence index in the current (possibly reduced) batch
-        seen = (sent << 32) + unfin_idx
-        unique_seen: List[int] = torch.unique(seen).tolist()
-
-        if self.match_source_len:
-            condition = step > torch.index_select(src_lengths, 0, unfin_idx)
-            eos_scores = torch.where(condition, torch.tensor(-math.inf), eos_scores)
-        sent_list: List[int] = sent.tolist()
-        for i in range(bbsz_idx.size()[0]):
-            # An input sentence (among those in a batch) is finished when
-            # beam_size hypotheses have been collected for it
-            if len(finalized[sent_list[i]]) < beam_size:
-                if attn_clone is not None:
-                    # remove padding tokens from attn scores
-                    hypo_attn = attn_clone[i]
-                else:
-                    hypo_attn = torch.empty(0)
-
-                finalized[sent_list[i]].append(
-                    {
-                        "tokens": tokens_clone[i],
-                        "score": eos_scores[i],
-                        "attention": hypo_attn,  # src_len x tgt_len
-                        "alignment": torch.empty(0),
-                        "positional_scores": pos_scores[i],
-                    }
-                )
-
-        newly_finished: List[int] = []
-        for unique_s in unique_seen:
-            # check termination conditions for this sentence
-            unique_sent: int = unique_s >> 32
-            unique_unfin_idx: int = unique_s - (unique_sent << 32)
-
-            if not finished[unique_sent] and self.is_finished(
-                step, unique_unfin_idx, max_len, len(finalized[unique_sent]), beam_size
-            ):
-                finished[unique_sent] = True
-                newly_finished.append(unique_unfin_idx)
-
-        return newly_finished
-
-    def is_finished(
-        self,
-        step: int,
-        unfin_idx: int,
-        max_len: int,
-        finalized_sent_len: int,
-        beam_size: int,
-    ):
-        """
-        Check whether decoding for a sentence is finished, which
-        occurs when the list of finalized sentences has reached the
-        beam size, or when we reach the maximum length.
-        """
-        assert finalized_sent_len <= beam_size
-        if finalized_sent_len == beam_size or step == max_len:
-            return True
-        return False
-
-
-class EnsembleModel(nn.Module):
-    """A wrapper around an ensemble of models."""
-
-    def __init__(self, models):
-        super().__init__()
-        self.models_size = len(models)
-        # method '__len__' is not supported in ModuleList for torch script
-        self.single_model = models[0]
-        self.models = nn.ModuleList(models)
-
-        self.has_incremental: bool = False
-        if all(
-            hasattr(m, "decoder") and isinstance(m.decoder, FairseqIncrementalDecoder)
-            for m in models
-        ):
-            self.has_incremental = True
-
-    def forward(self):
-        pass
-
-    def has_encoder(self):
-        return hasattr(self.single_model, "encoder")
-
-    def has_incremental_states(self):
-        return self.has_incremental
-
-    def max_decoder_positions(self):
-        return min([m.max_decoder_positions() for m in self.models if hasattr(m, "max_decoder_positions")] + [sys.maxsize])
-
-    @torch.jit.export
-    def forward_encoder(self, net_input: Dict[str, Tensor]):
-        if not self.has_encoder():
-            return None
-        return [model.encoder.forward_torchscript(net_input) for model in self.models]
-
-    @torch.jit.export
-    def forward_decoder(
-        self,
-        tokens,
-        encoder_outs: List[Dict[str, List[Tensor]]],
-        incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]],
-        temperature: float = 1.0,
-    ):
-        log_probs = []
-        avg_attn: Optional[Tensor] = None
-        encoder_out: Optional[Dict[str, List[Tensor]]] = None
-        for i, model in enumerate(self.models):
-            if self.has_encoder():
-                encoder_out = encoder_outs[i]
-            # decode each model
-            if self.has_incremental_states():
-                decoder_out = model.decoder.forward(
-                    tokens,
-                    encoder_out=encoder_out,
-                    incremental_state=incremental_states[i],
-                )
-            else:
-                if hasattr(model, "decoder"):
-                    decoder_out = model.decoder.forward(tokens, encoder_out=encoder_out)
-                else:
-                    decoder_out = model.forward(tokens)
-
-            attn: Optional[Tensor] = None
-            decoder_len = len(decoder_out)
-            if decoder_len > 1 and decoder_out[1] is not None:
-                if isinstance(decoder_out[1], Tensor):
-                    attn = decoder_out[1]
-                else:
-                    attn_holder = decoder_out[1]["attn"]
-                    if isinstance(attn_holder, Tensor):
-                        attn = attn_holder
-                    elif attn_holder is not None:
-                        attn = attn_holder[0]
-                if attn is not None:
-                    attn = attn[:, -1, :]
-
-            decoder_out_tuple = (
-                decoder_out[0][:, -1:, :].div_(temperature),
-                None if decoder_len <= 1 else decoder_out[1],
-            )
-            probs = model.get_normalized_probs(
-                decoder_out_tuple, log_probs=True, sample=None
-            )
-            probs = probs[:, -1, :]
-            if self.models_size == 1:
-                return probs, attn
-
-            log_probs.append(probs)
-            if attn is not None:
-                if avg_attn is None:
-                    avg_attn = attn
-                else:
-                    avg_attn.add_(attn)
-
-        avg_probs = torch.logsumexp(torch.stack(log_probs, dim=0), dim=0) - math.log(
-            self.models_size
-        )
-
-        if avg_attn is not None:
-            avg_attn.div_(self.models_size)
-        return avg_probs, avg_attn
-
-    @torch.jit.export
-    def reorder_encoder_out(
-        self, encoder_outs: Optional[List[Dict[str, List[Tensor]]]], new_order
-    ):
-        """
-        Reorder encoder output according to *new_order*.
-
-        Args:
-            encoder_out: output from the ``forward()`` method
-            new_order (LongTensor): desired order
-
-        Returns:
-            *encoder_out* rearranged according to *new_order*
-        """
-        new_outs: List[Dict[str, List[Tensor]]] = []
-        if not self.has_encoder():
-            return new_outs
-        for i, model in enumerate(self.models):
-            assert encoder_outs is not None
-            new_outs.append(
-                model.encoder.reorder_encoder_out(encoder_outs[i], new_order)
-            )
-        return new_outs
-
-    @torch.jit.export
-    def reorder_incremental_state(
-        self,
-        incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]],
-        new_order,
-    ):
-        if not self.has_incremental_states():
-            return
-        for i, model in enumerate(self.models):
-            model.decoder.reorder_incremental_state_scripting(
-                incremental_states[i], new_order
-            )
-
-
-class SequenceGeneratorWithAlignment(SequenceGenerator):
-    def __init__(
-        self, models, tgt_dict, left_pad_target=False, print_alignment="hard", **kwargs
-    ):
-        """Generates translations of a given source sentence.
-
-        Produces alignments following "Jointly Learning to Align and
-        Translate with Transformer Models" (Garg et al., EMNLP 2019).
-
-        Args:
-            left_pad_target (bool, optional): Whether or not the
-                hypothesis should be left padded or not when they are
-                teacher forced for generating alignments.
-        """
-        super().__init__(EnsembleModelWithAlignment(models), tgt_dict, **kwargs)
-        self.left_pad_target = left_pad_target
-
-        if print_alignment == "hard":
-            self.extract_alignment = utils.extract_hard_alignment
-        elif print_alignment == "soft":
-            self.extract_alignment = utils.extract_soft_alignment
-
-    @torch.no_grad()
-    def generate(self, models, sample, **kwargs):
-        finalized = super()._generate(sample, **kwargs)
-
-        src_tokens = sample["net_input"]["src_tokens"]
-        bsz = src_tokens.shape[0]
-        beam_size = self.beam_size
-        (
-            src_tokens,
-            src_lengths,
-            prev_output_tokens,
-            tgt_tokens,
-        ) = self._prepare_batch_for_alignment(sample, finalized)
-        if any(getattr(m, "full_context_alignment", False) for m in self.model.models):
-            attn = self.model.forward_align(src_tokens, src_lengths, prev_output_tokens)
-        else:
-            attn = [
-                finalized[i // beam_size][i % beam_size]["attention"].transpose(1, 0)
-                for i in range(bsz * beam_size)
-            ]
-
-        if src_tokens.device != "cpu":
-            src_tokens = src_tokens.to("cpu")
-            tgt_tokens = tgt_tokens.to("cpu")
-            attn = [i.to("cpu") for i in attn]
-
-        # Process the attn matrix to extract hard alignments.
-        for i in range(bsz * beam_size):
-            alignment = self.extract_alignment(
-                attn[i], src_tokens[i], tgt_tokens[i], self.pad, self.eos
-            )
-            finalized[i // beam_size][i % beam_size]["alignment"] = alignment
-        return finalized
-
-    def _prepare_batch_for_alignment(self, sample, hypothesis):
-        src_tokens = sample["net_input"]["src_tokens"]
-        bsz = src_tokens.shape[0]
-        src_tokens = (
-            src_tokens[:, None, :]
-            .expand(-1, self.beam_size, -1)
-            .contiguous()
-            .view(bsz * self.beam_size, -1)
-        )
-        src_lengths = sample["net_input"]["src_lengths"]
-        src_lengths = (
-            src_lengths[:, None]
-            .expand(-1, self.beam_size)
-            .contiguous()
-            .view(bsz * self.beam_size)
-        )
-        prev_output_tokens = data_utils.collate_tokens(
-            [beam["tokens"] for example in hypothesis for beam in example],
-            self.pad,
-            self.eos,
-            self.left_pad_target,
-            move_eos_to_beginning=True,
-        )
-        tgt_tokens = data_utils.collate_tokens(
-            [beam["tokens"] for example in hypothesis for beam in example],
-            self.pad,
-            self.eos,
-            self.left_pad_target,
-            move_eos_to_beginning=False,
-        )
-        return src_tokens, src_lengths, prev_output_tokens, tgt_tokens
-
-
-class EnsembleModelWithAlignment(EnsembleModel):
-    """A wrapper around an ensemble of models."""
-
-    def __init__(self, models):
-        super().__init__(models)
-
-    def forward_align(self, src_tokens, src_lengths, prev_output_tokens):
-        avg_attn = None
-        for model in self.models:
-            decoder_out = model(src_tokens, src_lengths, prev_output_tokens)
-            attn = decoder_out[1]["attn"][0]
-            if avg_attn is None:
-                avg_attn = attn
-            else:
-                avg_attn.add_(attn)
-        if len(self.models) > 1:
-            avg_attn.div_(len(self.models))
-        return avg_attn
diff --git a/fairseq/sequence_generator.py b/fairseq/sequence_generator.py
new file mode 120000
index 00000000..4422c4fd
--- /dev/null
+++ b/fairseq/sequence_generator.py
@@ -0,0 +1 @@
+/project/OML/dliu/src/adaptive-knn-mt/fairseq/sequence_generator.py
\ No newline at end of file
diff --git a/fairseq/tasks/translation.py b/fairseq/tasks/translation.py
index 86473608..ade6bd6b 100644
--- a/fairseq/tasks/translation.py
+++ b/fairseq/tasks/translation.py
@@ -393,6 +393,17 @@ class TranslationTask(FairseqTask):
                 logging_output["_bleu_totals_" + str(i)] = bleu.totals[i]
         return loss, sample_size, logging_output
 
+    def forward_and_get_hidden_state_step(self, sample, model):
+        # add by  
+        # forward the model with the sample, and get the decoder hidden state used for datastore
+        # and we only need the feature
+        decoder_output, extra = model(src_tokens=sample['net_input']['src_tokens'],
+                                      src_lengths=sample['net_input']['src_lengths'],
+                                      prev_output_tokens=sample['net_input']['prev_output_tokens'],
+                                      return_all_hiddens=False,
+                                      features_only=True)
+        return decoder_output
+
     def reduce_metrics(self, logging_outputs, criterion):
         super().reduce_metrics(logging_outputs, criterion)
         if self.cfg.eval_bleu:
diff --git a/fairseq/tasks/translation_multi_simple_epoch.py b/fairseq/tasks/translation_multi_simple_epoch.py
index 6f36e5b9..4b213217 100644
--- a/fairseq/tasks/translation_multi_simple_epoch.py
+++ b/fairseq/tasks/translation_multi_simple_epoch.py
@@ -4,6 +4,7 @@
 # LICENSE file in the root directory of this source tree.
 
 import datetime
+import itertools
 import logging
 import time
 
@@ -18,6 +19,7 @@ from fairseq.data import (
 from fairseq.data.multilingual.multilingual_data_manager import (
     MultilingualDatasetManager,
 )
+from fairseq.data.multilingual.multilingual_utils import LangTokStyle, get_lang_tok
 from fairseq.data.multilingual.sampling_method import SamplingMethod
 from fairseq.tasks import LegacyFairseqTask, register_task
 from fairseq.utils import FileContentsAction
@@ -72,6 +74,8 @@ class TranslationMultiSimpleEpochTask(LegacyFairseqTask):
                             action=FileContentsAction)
         parser.add_argument('--keep-inference-langtok', action='store_true',
                             help='keep language tokens in inference output (e.g. for analysis or debugging)')
+        parser.add_argument('--one-dataset-per-batch', action='store_true',
+                            help='limit each minibatch to one sub-dataset (typically lang direction)')
 
         SamplingMethod.add_arguments(parser)
         MultilingualDatasetManager.add_args(parser)
@@ -104,6 +108,19 @@ class TranslationMultiSimpleEpochTask(LegacyFairseqTask):
         self.data_manager = MultilingualDatasetManager.setup_data_manager(
             args, self.lang_pairs, langs, dicts, self.sampling_method
         )
+        self.lang_idx = self.get_lang_idx()
+        self.one_dataset_per_batch = getattr(args, "one_dataset_per_batch", False)
+
+    def get_lang_idx(self):
+        lang_idx = torch.zeros(len(self.langs) + 1, dtype=torch.int32)
+        # idx 0 for non-matching prefix tokens
+        lang_idx[0] = -1
+        for i, lang in enumerate(self.langs):
+            lang_tok = get_lang_tok(lang, LangTokStyle.multilingual.value)
+            lang_idx[i + 1] = MultilingualDatasetManager.get_langtok_index(
+                lang_tok, self.source_dictionary
+            )
+        return lang_idx
 
     def check_dicts(self, dicts, source_langs, target_langs):
         if self.args.source_dict is not None or self.args.target_dict is not None:
@@ -256,6 +273,19 @@ class TranslationMultiSimpleEpochTask(LegacyFairseqTask):
                     else self.target_dictionary.eos(),
                 )
 
+    def forward_and_get_hidden_state_step(self, sample, model):
+        # add by  
+        # forward the model with the sample, and get the decoder hidden state used for datastore
+        # and we only need the feature
+        decoder_output, extra = model(src_tokens=sample['net_input']['src_tokens'],
+                                      src_lengths=sample['net_input']['src_lengths'],
+                                      prev_output_tokens=sample['net_input']['prev_output_tokens'],
+                                      return_all_hiddens=False,
+                                      features_only=True,
+                                      src_lang_id=sample["net_input"]["src_lang_id"],
+                                      tgt_lang_id=sample["net_input"]["tgt_lang_id"])
+        return decoder_output
+
     def reduce_metrics(self, logging_outputs, criterion):
         super().reduce_metrics(logging_outputs, criterion)
 
@@ -293,43 +323,93 @@ class TranslationMultiSimpleEpochTask(LegacyFairseqTask):
             # get indices ordered by example size
             start_time = time.time()
             logger.info(f"start batch sampler: mem usage: {data_utils.get_mem_usage()}")
-
             with data_utils.numpy_seed(seed):
-                indices = dataset.ordered_indices()
-            logger.info(
-                f"[{split}] @batch_sampler order indices time: {get_time_gap(start_time, time.time())}"
-            )
-            logger.info(f"mem usage: {data_utils.get_mem_usage()}")
+                if self.one_dataset_per_batch:
+                    ordered_indices_list = dataset.ordered_indices_per_dataset()
+                else:
+                    ordered_indices_list = [dataset.ordered_indices()]
 
-            # filter examples that are too large
-            if max_positions is not None:
+            # get batches constructed from each underlying dataset to concatenate
+            subdataset_sampler_list = []
+            for ds_idx, indices in enumerate(ordered_indices_list):
+                if self.one_dataset_per_batch:
+                    log_tag = f"[{split}] [{ds_idx}]"
+                else:
+                    log_tag = f"[{split}]"
+                logger.info(
+                    f"{log_tag} @batch_sampler order indices time: {get_time_gap(start_time, time.time())}"
+                )
+                logger.info(f"mem usage: {data_utils.get_mem_usage()}")
+
+                # filter examples that are too large
+                if max_positions is not None and split is not None:
+                    my_time = time.time()
+                    indices = self.filter_indices_by_size(
+                        indices, dataset, max_positions, ignore_invalid_inputs
+                    )
+                    logger.info(
+                        f"{log_tag} @batch_sampler filter_by_size time: {get_time_gap(my_time, time.time())}"
+                    )
+                    logger.info(f"mem usage: {data_utils.get_mem_usage()}")
+
+                # create mini-batches with given size constraints
                 my_time = time.time()
-                indices = self.filter_indices_by_size(
-                    indices, dataset, max_positions, ignore_invalid_inputs
+                batch_sampler = dataset.batch_by_size(
+                    indices,
+                    max_tokens=max_tokens,
+                    max_sentences=max_sentences,
+                    required_batch_size_multiple=required_batch_size_multiple,
                 )
+                subdataset_sampler_list.append(batch_sampler)
+
+                end_time = time.time()
                 logger.info(
-                    f"[{split}] @batch_sampler filter_by_size time: {get_time_gap(my_time, time.time())}"
+                    f"{log_tag} @batch_sampler batch_by_size time: {get_time_gap(my_time, end_time)}"
+                )
+                logger.info(
+                    f"{log_tag} per epoch batch_sampler set-up time: {get_time_gap(start_time, end_time)}"
                 )
                 logger.info(f"mem usage: {data_utils.get_mem_usage()}")
 
-            # create mini-batches with given size constraints
-            my_time = time.time()
-            batch_sampler = dataset.batch_by_size(
-                indices,
-                max_tokens=max_tokens,
-                max_sentences=max_sentences,
-                required_batch_size_multiple=required_batch_size_multiple,
-            )
-
-            logger.info(
-                f"[{split}] @batch_sampler batch_by_size time: {get_time_gap(my_time, time.time())}"
-            )
-            logger.info(
-                f"[{split}] per epoch batch_sampler set-up time: {get_time_gap(start_time, time.time())}"
-            )
-            logger.info(f"mem usage: {data_utils.get_mem_usage()}")
+            combined_batch_sampler = itertools.chain(*subdataset_sampler_list)
+            return combined_batch_sampler
 
-            return batch_sampler
+#            with data_utils.numpy_seed(seed):
+#                indices = dataset.ordered_indices()
+#            logger.info(
+#                f"[{split}] @batch_sampler order indices time: {get_time_gap(start_time, time.time())}"
+#            )
+#            logger.info(f"mem usage: {data_utils.get_mem_usage()}")
+#
+#            # filter examples that are too large
+#            if max_positions is not None:
+#                my_time = time.time()
+#                indices = self.filter_indices_by_size(
+#                    indices, dataset, max_positions, ignore_invalid_inputs
+#                )
+#                logger.info(
+#                    f"[{split}] @batch_sampler filter_by_size time: {get_time_gap(my_time, time.time())}"
+#                )
+#                logger.info(f"mem usage: {data_utils.get_mem_usage()}")
+#
+#            # create mini-batches with given size constraints
+#            my_time = time.time()
+#            batch_sampler = dataset.batch_by_size(
+#                indices,
+#                max_tokens=max_tokens,
+#                max_sentences=max_sentences,
+#                required_batch_size_multiple=required_batch_size_multiple,
+#            )
+#
+#            logger.info(
+#                f"[{split}] @batch_sampler batch_by_size time: {get_time_gap(my_time, time.time())}"
+#            )
+#            logger.info(
+#                f"[{split}] per epoch batch_sampler set-up time: {get_time_gap(start_time, time.time())}"
+#            )
+#            logger.info(f"mem usage: {data_utils.get_mem_usage()}")
+#
+#            return batch_sampler
 
         return construct_batch_sampler
 
diff --git a/fairseq/trainer.py b/fairseq/trainer.py
index a10ec97a..c5909206 100644
--- a/fairseq/trainer.py
+++ b/fairseq/trainer.py
@@ -89,6 +89,7 @@ class Trainer(object):
         # copy model and criterion to current device/dtype
         self._criterion = criterion
         self._model = model
+
         if not self.is_fsdp:
             if cfg.common.fp16:
                 assert not cfg.common.amp, "Cannot use fp16 and AMP together"
@@ -506,7 +507,7 @@ class Trainer(object):
             # load model parameters
             try:
                 self.model.load_state_dict(
-                    state["model"], strict=True, model_cfg=self.cfg.model
+                    state["model"], strict=False, model_cfg=self.cfg.model
                 )
                 self.model.update_metadata(getattr(state["model"], "_metadata", None))
                 # save memory for later steps
diff --git a/fairseq_cli/interactive.py b/fairseq_cli/interactive.py
index cadef282..d2ee8bd3 100644
--- a/fairseq_cli/interactive.py
+++ b/fairseq_cli/interactive.py
@@ -223,6 +223,13 @@ def main(cfg: FairseqConfig):
                     "src_lengths": src_lengths,
                 },
             }
+            # for adapters
+            if cfg.task.enable_lang_ids and cfg.task.one_dataset_per_batch:
+                src_lang_id = task.data_manager.lang_dict.indices[task.source_langs[0]]
+                tgt_lang_id = task.data_manager.lang_dict.indices[task.target_langs[0]]
+                sample["net_input"]["src_lang_id"] = src_tokens.new([src_lang_id]).repeat(src_tokens.size(0), 1)
+                sample["net_input"]["tgt_lang_id"] = src_tokens.new([tgt_lang_id]).repeat(src_tokens.size(0), 1)
+
             translate_start_time = time.time()
             translations = task.inference_step(
                 generator, models, sample, constraints=constraints
